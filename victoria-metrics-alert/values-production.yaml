server:
  datasource:
    url: "http://vm-cluster-victoria-metrics-cluster-vmselect:8481/select/0/prometheus/"

  replicaCount: 1

  notifier:
    alertmanager:
      url: "http://vm-alertmanager:9093"

  resources:
    limits:
      cpu: 400m
      memory: 512Mi
    requests:
      cpu: 100m
      memory: 128Mi

  podAnnotations:
      prometheus.io/scrape: "true"
      prometheus.io/port: "8880"

  config:
    alerts:
      groups:
        - name: common
          rules:
            - alert: instanceIsDown
              for: 1m
              expr: up == 0
              labels:
                severity: warning
              annotations:
                summary: "{{ $labels.job }} instance: {{$labels.instance }} is not up"
                description: "Job {{ $labels.job }}  instance: {{$labels.instance }} is not up for the last 1 minute"
            - alert: HostOutOfMemory
              expr: (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes * 100 < 10) * on(instance) group_left (nodename) node_uname_info{nodename=~".+"}
              for: 2m
              labels:
                severity: warning
              annotations:
                summary: Host out of memory (instance {{ $labels.instance }})
                description: "Node memory is filling up (< 10% left)\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
            - alert: HostMemoryUnderMemoryPressure
              expr: (rate(node_vmstat_pgmajfault[1m]) > 1000) * on(instance) group_left (nodename) node_uname_info{nodename=~".+"}
              for: 2m
              labels:
                severity: warning
              annotations:
                summary: Host memory under memory pressure (instance {{ $labels.instance }})
                description: "The node is under heavy memory pressure. High rate of major page faults\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
            # You may want to increase the alert manager 'repeat_interval' for this type of alert to daily or weekly
            - alert: HostMemoryIsUnderutilized
              expr: (100 - (avg_over_time(node_memory_MemAvailable_bytes[30m]) / node_memory_MemTotal_bytes * 100) < 20) * on(instance) group_left (nodename) node_uname_info{nodename=~".+"}
              for: 1w
              labels:
                severity: info
              annotations:
                summary: Host Memory is underutilized (instance {{ $labels.instance }})
                description: "Node memory is < 20% for 1 week. Consider reducing memory space. (instance {{ $labels.instance }})\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
            - alert: HostUnusualNetworkThroughputIn
              expr: (sum by (instance) (rate(node_network_receive_bytes_total[2m])) / 1024 / 1024 > 100) * on(instance) group_left (nodename) node_uname_info{nodename=~".+"}
              for: 5m
              labels:
                severity: warning
              annotations:
                summary: Host unusual network throughput in (instance {{ $labels.instance }})
                description: "Host network interfaces are probably receiving too much data (> 100 MB/s)\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
            - alert: HostUnusualNetworkThroughputOut
              expr: (sum by (instance) (rate(node_network_transmit_bytes_total[2m])) / 1024 / 1024 > 100) * on(instance) group_left (nodename) node_uname_info{nodename=~".+"}
              for: 5m
              labels:
                severity: warning
              annotations:
                summary: Host unusual network throughput out (instance {{ $labels.instance }})
                description: "Host network interfaces are probably sending too much data (> 100 MB/s)\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
            - alert: HostUnusualDiskReadRate
              expr: (sum by (instance) (rate(node_disk_read_bytes_total[2m])) / 1024 / 1024 > 50) * on(instance) group_left (nodename) node_uname_info{nodename=~".+"}
              for: 5m
              labels:
                severity: warning
              annotations:
                summary: Host unusual disk read rate (instance {{ $labels.instance }})
                description: "Disk is probably reading too much data (> 50 MB/s)\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
            - alert: HostUnusualDiskWriteRate
              expr: (sum by (instance) (rate(node_disk_written_bytes_total[2m])) / 1024 / 1024 > 50) * on(instance) group_left (nodename) node_uname_info{nodename=~".+"}
              for: 2m
              labels:
                severity: warning
              annotations:
                summary: Host unusual disk write rate (instance {{ $labels.instance }})
                description: "Disk is probably writing too much data (> 50 MB/s)\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
            - alert: HostOutOfDiskSpace
              expr: ((node_filesystem_avail_bytes * 100) / node_filesystem_size_bytes < 10 and ON (instance, device, mountpoint) node_filesystem_readonly == 0) * on(instance) group_left (nodename) node_uname_info{nodename=~".+"}
              for: 2m
              labels:
                severity: warning
              annotations:
                summary: Host out of disk space (instance {{ $labels.instance }})
                description: "Disk is almost full (< 10% left)\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
            - alert: HostDiskWillFillIn24Hours
              expr: ((node_filesystem_avail_bytes * 100) / node_filesystem_size_bytes < 10 and ON (instance, device, mountpoint) predict_linear(node_filesystem_avail_bytes{fstype!~"tmpfs"}[1h], 24 * 3600) < 0 and ON (instance, device, mountpoint) node_filesystem_readonly == 0) * on(instance) group_left (nodename) node_uname_info{nodename=~".+"}
              for: 2m
              labels:
                severity: warning
              annotations:
                summary: Host disk will fill in 24 hours (instance {{ $labels.instance }})
                description: "Filesystem is predicted to run out of space within the next 24 hours at current write rate\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
            - alert: HostOutOfInodes
              expr: (node_filesystem_files_free{fstype!="msdosfs"} / node_filesystem_files{fstype!="msdosfs"} * 100 < 10 and ON (instance, device, mountpoint) node_filesystem_readonly == 0) * on(instance) group_left (nodename) node_uname_info{nodename=~".+"}
              for: 2m
              labels:
                severity: warning
              annotations:
                summary: Host out of inodes (instance {{ $labels.instance }})
                description: "Disk is almost running out of available inodes (< 10% left)\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
            - alert: HostFilesystemDeviceError
              expr: node_filesystem_device_error == 1
              for: 2m
              labels:
                severity: warning
              annotations:
                summary: Host filesystem device error (instance {{ $labels.instance }})
                description: "{{ $labels.instance }}: Device error with the {{ $labels.mountpoint }} filesystem\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
            - alert: HostInodesWillFillIn24Hours
              expr: (node_filesystem_files_free{fstype!="msdosfs"} / node_filesystem_files{fstype!="msdosfs"} * 100 < 10 and predict_linear(node_filesystem_files_free{fstype!="msdosfs"}[1h], 24 * 3600) < 0 and ON (instance, device, mountpoint) node_filesystem_readonly{fstype!="msdosfs"} == 0) * on(instance) group_left (nodename) node_uname_info{nodename=~".+"}
              for: 2m
              labels:
                severity: warning
              annotations:
                summary: Host inodes will fill in 24 hours (instance {{ $labels.instance }})
                description: "Filesystem is predicted to run out of inodes within the next 24 hours at current write rate\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
            - alert: HostUnusualDiskReadLatency
              expr: (rate(node_disk_read_time_seconds_total[1m]) / rate(node_disk_reads_completed_total[1m]) > 0.1 and rate(node_disk_reads_completed_total[1m]) > 0) * on(instance) group_left (nodename) node_uname_info{nodename=~".+"}
              for: 2m
              labels:
                severity: warning
              annotations:
                summary: Host unusual disk read latency (instance {{ $labels.instance }})
                description: "Disk latency is growing (read operations > 100ms)\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
            - alert: HostUnusualDiskWriteLatency
              expr: (rate(node_disk_write_time_seconds_total[1m]) / rate(node_disk_writes_completed_total[1m]) > 0.1 and rate(node_disk_writes_completed_total[1m]) > 0) * on(instance) group_left (nodename) node_uname_info{nodename=~".+"}
              for: 2m
              labels:
                severity: warning
              annotations:
                summary: Host unusual disk write latency (instance {{ $labels.instance }})
                description: "Disk latency is growing (write operations > 100ms)\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
            - alert: HostHighCpuLoad
              expr: (sum by (instance) (avg by (mode, instance) (rate(node_cpu_seconds_total{mode!="idle"}[2m]))) > 0.8) * on(instance) group_left (nodename) node_uname_info{nodename=~".+"}
              for: 10m
              labels:
                severity: warning
              annotations:
                summary: Host high CPU load (instance {{ $labels.instance }})
                description: "CPU load is > 80%\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
            # You may want to increase the alert manager 'repeat_interval' for this type of alert to daily or weekly
            - alert: HostCpuIsUnderutilized
              expr: (100 - (rate(node_cpu_seconds_total{mode="idle"}[30m]) * 100) < 20) * on(instance) group_left (nodename) node_uname_info{nodename=~".+"}
              for: 1w
              labels:
                severity: info
              annotations:
                summary: Host CPU is underutilized (instance {{ $labels.instance }})
                description: "CPU load is < 20% for 1 week. Consider reducing the number of CPUs.\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
            - alert: HostCpuStealNoisyNeighbor
              expr: (avg by(instance) (rate(node_cpu_seconds_total{mode="steal"}[5m])) * 100 > 10) * on(instance) group_left (nodename) node_uname_info{nodename=~".+"}
              for: 0m
              labels:
                severity: warning
              annotations:
                summary: Host CPU steal noisy neighbor (instance {{ $labels.instance }})
                description: "CPU steal is > 10%. A noisy neighbor is killing VM performances or a spot instance may be out of credit.\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
            - alert: HostCpuHighIowait
              expr: (avg by (instance) (rate(node_cpu_seconds_total{mode="iowait"}[5m])) * 100 > 10) * on(instance) group_left (nodename) node_uname_info{nodename=~".+"}
              for: 0m
              labels:
                severity: warning
              annotations:
                summary: Host CPU high iowait (instance {{ $labels.instance }})
                description: "CPU iowait > 10%. A high iowait means that you are disk or network bound.\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
            - alert: HostUnusualDiskIo
              expr: (rate(node_disk_io_time_seconds_total[1m]) > 0.5) * on(instance) group_left (nodename) node_uname_info{nodename=~".+"}
              for: 5m
              labels:
                severity: warning
              annotations:
                summary: Host unusual disk IO (instance {{ $labels.instance }})
                description: "Time spent in IO is too high on {{ $labels.instance }}. Check storage for issues.\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
            # x2 context switches is an arbitrary number.
            # The alert threshold depends on the nature of the application.
            # Please read: https://github.com/samber/awesome-prometheus-alerts/issues/58
            - alert: HostContextSwitchingHigh
              expr: (rate(node_context_switches_total[15m])/count without(mode,cpu) (node_cpu_seconds_total{mode="idle"})) / (rate(node_context_switches_total[1d])/count without(mode,cpu) (node_cpu_seconds_total{mode="idle"})) > 2
              for: 0m
              labels:
                severity: warning
              annotations:
                summary: Host context switching high (instance {{ $labels.instance }})
                description: "Context switching is growing on the node (twice the daily average during the last 15m)\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
            - alert: HostSwapIsFillingUp
              expr: ((1 - (node_memory_SwapFree_bytes / node_memory_SwapTotal_bytes)) * 100 > 80) * on(instance) group_left (nodename) node_uname_info{nodename=~".+"}
              for: 2m
              labels:
                severity: warning
              annotations:
                summary: Host swap is filling up (instance {{ $labels.instance }})
                description: "Swap is filling up (>80%)\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
            - alert: HostSystemdServiceCrashed
              expr: (node_systemd_unit_state{state="failed"} == 1) * on(instance) group_left (nodename) node_uname_info{nodename=~".+"}
              for: 0m
              labels:
                severity: warning
              annotations:
                summary: Host systemd service crashed (instance {{ $labels.instance }})
                description: "systemd service crashed\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
            - alert: HostOomKillDetected
              expr: (increase(node_vmstat_oom_kill[1m]) > 0) * on(instance) group_left (nodename) node_uname_info{nodename=~".+"}
              for: 0m
              labels:
                severity: warning
              annotations:
                summary: Host OOM kill detected (instance {{ $labels.instance }})
                description: "OOM kill detected\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
            - alert: HostNetworkReceiveErrors
              expr: (rate(node_network_receive_errs_total[2m]) / rate(node_network_receive_packets_total[2m]) > 0.01) * on(instance) group_left (nodename) node_uname_info{nodename=~".+"}
              for: 2m
              labels:
                severity: warning
              annotations:
                summary: Host Network Receive Errors (instance {{ $labels.instance }})
                description: "Host {{ $labels.instance }} interface {{ $labels.device }} has encountered {{ printf \"%.0f\" $value }} receive errors in the last two minutes.\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
            - alert: HostNetworkTransmitErrors
              expr: (rate(node_network_transmit_errs_total[2m]) / rate(node_network_transmit_packets_total[2m]) > 0.01) * on(instance) group_left (nodename) node_uname_info{nodename=~".+"}
              for: 2m
              labels:
                severity: warning
              annotations:
                summary: Host Network Transmit Errors (instance {{ $labels.instance }})
                description: "Host {{ $labels.instance }} interface {{ $labels.device }} has encountered {{ printf \"%.0f\" $value }} transmit errors in the last two minutes.\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
            - alert: HostConntrackLimit
              expr: (node_nf_conntrack_entries / node_nf_conntrack_entries_limit > 0.8) * on(instance) group_left (nodename) node_uname_info{nodename=~".+"}
              for: 5m
              labels:
                severity: warning
              annotations:
                summary: Host conntrack limit (instance {{ $labels.instance }})
                description: "The number of conntrack is approaching limit\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
            - alert: HostClockSkew
              expr: ((node_timex_offset_seconds > 0.05 and deriv(node_timex_offset_seconds[5m]) >= 0) or (node_timex_offset_seconds < -0.05 and deriv(node_timex_offset_seconds[5m]) <= 0)) * on(instance) group_left (nodename) node_uname_info{nodename=~".+"}
              for: 10m
              labels:
                severity: warning
              annotations:
                summary: Host clock skew (instance {{ $labels.instance }})
                description: "Clock skew detected. Clock is out of sync. Ensure NTP is configured correctly on this host.\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
            - alert: HostClockNotSynchronising
              expr: (min_over_time(node_timex_sync_status[1m]) == 0 and node_timex_maxerror_seconds >= 16) * on(instance) group_left (nodename) node_uname_info{nodename=~".+"}
              for: 2m
              labels:
                severity: warning
              annotations:
                summary: Host clock not synchronising (instance {{ $labels.instance }})
                description: "Clock not synchronising. Ensure NTP is configured on this host.\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
            - alert: HostRequiresReboot
              expr: (node_reboot_required > 0) * on(instance) group_left (nodename) node_uname_info{nodename=~".+"}
              for: 4h
              labels:
                severity: info
              annotations:
                summary: Host requires reboot (instance {{ $labels.instance }})
                description: "{{ $labels.instance }} requires a reboot.\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

            # Docker containers : google/cAdvisor 
            - alert: ContainerKilled
              expr: time() - container_last_seen > 60
              for: 0m
              labels:
                severity: warning
              annotations:
                summary: Container killed (instance {{ $labels.instance }})
                description: "A container has disappeared\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
            - alert: ContainerHighCpuUtilization
              expr: (sum(rate(container_cpu_usage_seconds_total{container!=""}[5m])) by (pod, container) / sum(container_spec_cpu_quota{container!=""}/container_spec_cpu_period{container!=""}) by (pod, container) * 100) > 80
              for: 2m
              labels:
                severity: warning
              annotations:
                summary: Container High CPU utilization (instance {{ $labels.instance }})
                description: "Container CPU utilization is above 80%\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
            # See https://medium.com/faun/how-much-is-too-much-the-linux-oomkiller-and-used-memory-d32186f29c9d
            - alert: ContainerHighMemoryUsage
              expr: (sum(container_memory_working_set_bytes{name!=""}) BY (instance, name) / sum(container_spec_memory_limit_bytes > 0) BY (instance, name) * 100) > 80
              for: 2m
              labels:
                severity: warning
              annotations:
                summary: Container High Memory usage (instance {{ $labels.instance }})
                description: "Container Memory usage is above 80%\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
            - alert: ContainerVolumeUsage
              expr: (1 - (sum(container_fs_inodes_free{name!=""}) BY (instance) / sum(container_fs_inodes_total) BY (instance))) * 100 > 80
              for: 2m
              labels:
                severity: warning
              annotations:
                summary: Container Volume usage (instance {{ $labels.instance }})
                description: "Container Volume usage is above 80%\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
            - alert: ContainerHighThrottleRate
              expr: sum(increase(container_cpu_cfs_throttled_periods_total{container!=""}[5m])) by (container, pod, namespace) / sum(increase(container_cpu_cfs_periods_total[5m])) by (container, pod, namespace) > ( 25 / 100 )
              for: 5m
              labels:
                severity: warning
              annotations:
                summary: Container high throttle rate (instance {{ $labels.instance }})
                description: "Container is being throttled\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
